# -*- coding: utf-8 -*-
"""MDS18 Final Model

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RUI6kdtW6fmLxWhKHMFLnjftxZ07Eavq
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV, RepeatedKFold
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.feature_selection import SequentialFeatureSelector
from google.colab import files
from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import StackingRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.ensemble import ExtraTreesRegressor
import joblib
import doctest


import warnings
warnings.filterwarnings("ignore")

uploaded = files.upload()

df = pd.read_excel('STP bioeffluent dataset [Updated].xlsx',sheet_name='Sheet1')

# make a copy of the df before conversion
ori_df = df.copy()

# convert all columns to numeric, except 'Year' 'Month' 'Area'
df[df.columns[2:-1]] = df[df.columns[2:-1]].apply(pd.to_numeric, errors = 'coerce')

# before conversion
df.describe().round(2)

# Identify objects converted to NaN for each column
NaN = {}

for col in df.columns[2:-1]:
    converted = ori_df[col][ori_df[col].notnull() & df[col].isnull()].unique()
    if len(converted) > 0:
        NaN[col] = converted

for col, converted in NaN.items():
    print(f"{col}:")
    print(converted)

# determine which columns have minimum values smaller than the converted objects
# and replace NaN values with converted object divided by 2

for col, converted in NaN.items():
      min_value = df[col].min()
      for obj in converted:
          if min_value < float(obj.lstrip("<=")):
              print(f"Minimum valuen in '{col}' = {min_value} {obj}")

          # replace NaN values
          converted_value = float(obj.lstrip("<=")) / 2
          df[col].fillna(converted_value, inplace=True)

# after conversion
df.isna().sum()

df.dropna(inplace=True)

df = df[(df['COD'] < 186)]
corr_matrix = df.corr(numeric_only=True)

# plot the correlation matrix
plt.figure(figsize=(10, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
plt.title('Correlation Matrix')
plt.show()

# Drop features
df.drop(['Area', 'Year', 'Month'], axis=1, inplace=True)

# Separate features and targets
targets = df[['BOD', 'COD', 'Ammonia (NH3)', 'Nitrate (NO3)']]

train_sets = {}
test_sets = {}

for target in targets:
  features = df.drop([target], axis = 1)
  X_train, X_test, y_train, y_test = train_test_split(features, targets[target], test_size=0.25, random_state=42)

  train_sets[target] = {'X_train': X_train, 'y_train': y_train}
  test_sets[target] = {'X_test': X_test, 'y_test': y_test}

  # Print the training set
  print(f"Shape of Training and Testing sets for {target}:")
  print(X_train.shape)
  print(X_test.shape)

LR_models = {}

for target, train_set in train_sets.items():
  X_train = train_set['X_train']
  y_train = train_set['y_train']

 # Fit
  model = LinearRegression()
  model.fit(X_train, y_train)

  LR_models[target] = model

  train_score = model.score(X_train, y_train)

  print(f"Training score (R^2) of {target} : {train_score}", "\n")

# Predict
  X_test = test_sets[target]['X_test']

  y_test = test_sets[target]['y_test']
  y_pred = model.predict(X_test)

selected_train_sets = {}
selected_test_sets = {}

for target, train_set in train_sets.items():
    X_train = train_set['X_train']
    y_train = train_set['y_train']

    feature_names = X_train.columns.tolist()

    if target == 'BOD':
      selector = SequentialFeatureSelector(LinearRegression(), n_features_to_select = 3, direction = 'forward', scoring = 'r2', cv = 5)

    else:
      selector = SequentialFeatureSelector(LinearRegression(), n_features_to_select = 5 if target == 'COD' else 6, direction = 'forward', scoring = 'r2', cv = 5)

    selector.fit(X_train, y_train)

    # Get selected features
    if target == 'BOD':
      selected_features = X_train
    else:
      selected_features = selector.get_support()
    selected = []

    for i, feature in enumerate(selected_features):
        if feature:
            selected.append(feature_names[i])
    print(selected)

    X_test = test_sets[target]['X_test']
    y_test = test_sets[target]['y_test']

    if target != 'BOD':
      X_train = selector.transform(X_train)
      X_test = selector.transform(X_test)

    # Store X_train with only selected variables
    selected_train_sets[target] = {'X_train': X_train, 'y_train': y_train}

    model = LinearRegression()
    model.fit(X_train, y_train)

    selected_test_sets[target] = {'X_test': X_test, 'y_test': y_test}

    train_score = model.score(X_train, y_train)


    y_pred = model.predict(X_test)

models = {}

for target in ['BOD', 'Ammonia (NH3)', 'Nitrate (NO3)']:
  train_set = train_sets[target]
  X_train = train_set['X_train']
  y_train = train_set['y_train']

  if target == 'Nitrate (NO3)':
    model = RandomForestRegressor()

  else:
    model = ExtraTreesRegressor()

  model.fit(X_train, y_train)
  models[target] = model

train_set = selected_train_sets['COD']
X_train = train_set['X_train']
y_train = train_set['y_train']

knn = KNeighborsRegressor(algorithm='auto', leaf_size=50, n_neighbors=9, weights='distance')
knn.fit(X_train, y_train)

gbr = GradientBoostingRegressor(alpha=0.1, learning_rate=0.1, loss='huber', n_estimators=200)
gbr.fit(X_train, y_train)


y_pred_knn = knn.predict(X_train)
y_pred_gbr = gbr.predict(X_train)

stacked_features = np.column_stack((y_pred_knn, y_pred_gbr))
meta_model = LinearRegression()
meta_model.fit(stacked_features, y_train)

models['COD'] = {'knn': knn, 'gbr': gbr, 'meta_model': meta_model}
joblib.dump(models, 'model.pkl')

df_val = pd.read_excel('STP bioeffluent dataset [Updated].xlsx',sheet_name='Validation')
ori_val = df_val.copy()

# Preprocess the validation set
df_val[df_val.columns[2:-1]] = df_val[df_val.columns[2:-1]].apply(pd.to_numeric, errors = 'coerce')

# Identify objects converted to NaN for each column
NaN = {}

for col in df_val.columns[2:-1]:
    converted = ori_val[col][ori_val[col].notnull() & df_val[col].isnull()].unique()
    if len(converted) > 0:
        NaN[col] = converted

for col, converted in NaN.items():
      min_value = df_val[col].min()
      for obj in converted:
          if min_value < float(obj.lstrip("<=")):
              print(f"Minimum value in '{col}' = {min_value} {obj}")

          # replace NaN values
          converted_value = float(obj.lstrip("<=")) / 2
          df_val[col].fillna(converted_value, inplace=True)

df_val.drop(['Area', 'Year', 'Month'], axis=1, inplace=True)

def separate_xy(df):

  targets = df[['BOD', 'COD', 'Ammonia (NH3)', 'Nitrate (NO3)']]
  validation_sets = {}

  for target in targets:
    X_val = df.drop([target], axis = 1)
    y_val = df[[target]]

    validation_sets[target] = {'X_val': X_val, 'y_val': y_val}

  return validation_sets


without_na = separate_xy(df_val.dropna())
with_na = separate_xy(df_val[df_val.isna().any(axis=1)])
print(with_na["BOD"]["X_val"])

# split validation set into multiple random equal sets
def split_validation(df, sets_no):
  df_shuffled = df.sample(frac=1).reset_index(drop=True)
  sets = np.array_split(df_shuffled, sets_no)
  sets = [pd.DataFrame(split) for split in sets]
  return sets

def prediction(model_path, input):

    # Load models
    try:
      models = joblib.load(model_path)
      model_bod = models['BOD']
      model_nh3 = models['Ammonia (NH3)']
      model_no3 = models['Nitrate (NO3)']
      knn = models['COD']['knn']
      gbr = models['COD']['gbr']
      meta_model = models['COD']['meta_model']
    except Exception as e:
      return

    if isinstance(input, pd.DataFrame):
      input.dropna(inplace=True)

      columns_to_drop = []
      for column in input.columns:
        if input[column].dtype not in [np.float64, np.int64]:
          try:
            input[column] = input[column].astype(float)
          except (TypeError, ValueError):
            columns_to_drop.append(column)

      input.drop(columns = columns_to_drop, inplace = True)

      if not input.empty:

          # Preprocess user input
          header = input.columns.tolist()
          sub_header = ["COD", "pH", "Oil & Grease", "Suspended Solid", "Temp"]
          status = {'BOD': False, 'COD': False, 'Ammonia (NH3)': False, 'Nitrate (NO3)': False}

          # To store results dataframes
          results = []

          # COD
          cod_header = ['BOD', 'Ammonia (NH3)', 'Nitrate (NO3)', 'Oil & Grease', 'Suspended Solid']
          if all(item in header for item in cod_header):
              cod_input = input[cod_header]
              cod_features = np.column_stack([knn.predict(cod_input), gbr.predict(cod_input)])
              pred_cod = meta_model.predict(cod_features)
              pred_cod_df = pd.DataFrame(pred_cod, columns=['Predicted_COD'], index=cod_input.index)
              cod_results = pd.concat([cod_input, pred_cod_df], axis=1)
              results.append(cod_results)
              status['COD'] = True

          # Filter input (BOD, NH3, NO3)
          if all(item in header for item in sub_header):
              if all(item in input.columns for item in ['Ammonia (NH3)', 'Nitrate (NO3)']):
                  bod_header = ['COD', 'Ammonia (NH3)', 'Nitrate (NO3)'] + sub_header[1:]
                  bod_input = input[bod_header]
                  pred_bod = model_bod.predict(bod_input)
                  pred_bod_df = pd.DataFrame(pred_bod, columns=['Predicted_BOD'], index=bod_input.index)
                  results.append(pd.concat([bod_input, pred_bod_df], axis=1))
                  status['BOD'] = True

              if all(item in input.columns for item in ['BOD', 'Nitrate (NO3)']):
                  nh3_header = ['BOD', 'COD', 'Nitrate (NO3)'] + sub_header[1:]
                  nh3_input = input[nh3_header]
                  pred_nh3 = model_nh3.predict(nh3_input)
                  pred_nh3_df = pd.DataFrame(pred_nh3, columns=['Predicted_NH3'], index=nh3_input.index)
                  results.append(pd.concat([nh3_input, pred_nh3_df], axis=1))
                  status['Ammonia (NH3)'] = True

              if all(item in input.columns for item in ['BOD', 'Ammonia (NH3)']):
                  no3_header = ['BOD', 'COD', 'Ammonia (NH3)'] + sub_header[1:]
                  no3_input = input[no3_header]
                  pred_no3 = model_no3.predict(no3_input)
                  pred_no3_df = pd.DataFrame(pred_no3, columns=['Predicted_NO3'], index=no3_input.index)
                  results.append(pd.concat([no3_input, pred_no3_df], axis=1))
                  status['Nitrate (NO3)'] = True


          # Combine all results
          if results:
              final_results = pd.concat(results, axis=1)
              final_results = final_results.loc[:, ~final_results.columns.duplicated()]
              return final_results
          return None

prediction("model.pkl", df_val)

"""**Black Box Testing**"""

# Test Case 1: Valid Input with All Required Columns
def test_case_1():
    """
    Test Case 1: Valid Input with All Required Columns

    >>> result = prediction("model.pkl", df_val)
    >>> result is not None
    True
    >>> isinstance(result, pd.DataFrame)
    True
    >>> 'Predicted_BOD' in result.columns
    True
    >>> 'Predicted_COD' in result.columns
    True
    >>> 'Predicted_NH3' in result.columns
    True
    >>> 'Predicted_NO3' in result.columns
    True
    """
    pass

# Test Case 2 : Input Missing a Column
def test_case_2():
    """
    Test Case 2: Input missing a column

    >>> missing_bod = df_val.drop('BOD', axis=1)
    >>> result_bod = prediction("model.pkl", missing_bod)
    >>> 'Predicted_BOD' in result_bod.columns
    True

    >>> missing_cod = df_val.drop('COD', axis=1)
    >>> result_cod = prediction("model.pkl", missing_cod)
    >>> 'Predicted_COD' in result_cod.columns
    True

    >>> missing_nh3 = df_val.drop('Ammonia (NH3)', axis=1)
    >>> result_nh3 = prediction("model.pkl", missing_nh3)
    >>> 'Predicted_NH3' in result_nh3.columns
    True

    >>> missing_no3 = df_val.drop('Nitrate (NO3)', axis=1)
    >>> result_no3 = prediction("model.pkl", missing_no3)
    >>> 'Predicted_NO3' in result_no3.columns
    True
    """
    pass

# Test Case 3 : Input Missing Some Columns
def test_case_3():
    """
    Test Case 3: Input missing some columns

    >>> df = df_val.drop(columns=['Temp', 'pH'])
    >>> result = prediction("model.pkl", df)
    >>> 'Predicted_COD' in result.columns
    True
    >>> 'Predicted_BOD' not in result.columns
    True
    >>> 'Predicted_NH3' not in result.columns
    True
    >>> 'Predicted_NO3' not in result.columns
    True
    """
    pass

# Test Case 4 : Input Missing Some Values
def test_case_4():
    """
    Test Case 4: Input Missing Some Values

    >>> result = prediction("model.pkl", df_val)
    >>> result is not None
    True
    >>> isinstance(result, pd.DataFrame)
    True
    >>> 'Predicted_BOD' in result.columns
    True
    >>> 'Predicted_COD' in result.columns
    True
    >>> 'Predicted_NH3' in result.columns
    True
    >>> 'Predicted_NO3' in result.columns
    True
    """
    pass

# Test Case 5 : Empty Input
def test_case_5():
    """
    Test Case 5: Empty Input

    >>> empty_df = pd.DataFrame()
    >>> result = prediction("model.pkl", empty_df)
    >>> result is None
    True
    """
    pass

# Test Case 6 : Invalid Model Path
def test_case_6():
    """
    Test Case 6: Invalid Model Path

    >>> result = prediction("invalid_path.pkl", df_val)
    >>> result is None
    True
    """
    pass

# Test Case 7 : Input as non-data frame type
def test_case_7():
    """
    Test Case 7: Data Type of Input

    >>> array = df_val.values
    >>> result = prediction("model.pkl", array)
    >>> result is None
    True
    """
    pass


doctest.testmod()

"""**Integration Testing**"""

# Integration Test for Preprocessing
# Test Case 1 : Input with unrelated Columns
def IT_test_case_1():
    """
    Test Case 1: Input with unrelated Columns

    >>> df = df_val.copy()
    >>> df['Dummy_Column'] = np.random.rand(df.shape[0])
    >>> result = prediction("model.pkl", df)
    >>> result is not None
    True
    >>> isinstance(result, pd.DataFrame)
    True
    >>> 'Predicted_BOD' in result.columns
    True
    >>> 'Predicted_COD' in result.columns
    True
    >>> 'Predicted_NH3' in result.columns
    True
    >>> 'Predicted_NO3' in result.columns
    True
    """
    pass

# Test Case 2 : Input with columns in random order
def IT_test_case_2():

    """
    Test Case 2 : Input with columns in random order

    >>> df = df_val.copy()
    >>> shuffle_df = np.random.permutation(df.columns)
    >>> df = df[shuffle_df]
    >>> result = prediction("model.pkl", df)
    >>> result is not None
    True
    >>> isinstance(result, pd.DataFrame)
    True
    >>> 'Predicted_BOD' in result.columns
    True
    >>> 'Predicted_COD' in result.columns
    True
    >>> 'Predicted_NH3' in result.columns
    True
    >>> 'Predicted_NO3' in result.columns
    True
    """
    pass

# Test Case 3 : Verify that Output Columns are not Duplicated
def test_case_3():

    """
    Test Case 3 : Verify that Output Columns are not Duplicated

    >>> result = prediction("model.pkl", df_val)
    >>> result is not None
    True
    >>> len(result.columns) == len(set(result.columns))
    True
    """
    pass

# Integration tests to vaidate the interaction between the 'prediction' function and the individual prediction models
# Test Case 4 : Integration Test for BOD model
def IT_test_case_4():
    """
    Test Case 4 : Integration Test for BOD model

    >>> result = prediction("model.pkl", df_val)
    >>> result is not None
    True
    >>> 'Predicted_BOD' in result.columns
    True
    >>> (result['Predicted_BOD'] >= 0).all()
    True
    """

# Test Case 5 : Integration Test for NH3 model
def IT_test_case_5():
    """
    Test Case 5 : Integration Test for NH3 model

    >>> result = prediction("model.pkl", df_val)
    >>> result is not None
    True
    >>> 'Predicted_NH3' in result.columns
    True
    >>> (result['Predicted_NH3'] >= 0).all()
    True
    """
# Test Case 6 : Integration Test for NO3 model
def IT_test_case_6():
    """
    Test Case 6 : Integration Test for NO3 model

    >>> result = prediction("model.pkl", df_val)
    >>> result is not None
    True
    >>> 'Predicted_NO3' in result.columns
    True
    >>> (result['Predicted_NO3'] >= 0).all()
    True
    """

# Test Case 7 : Integration Test for COD model
def IT_test_case_7():
    """
    Test Case 7 : Integration Test for COD model

    >>> result = prediction("model.pkl", df_val)
    >>> result is not None
    True
    >>> 'Predicted_COD' in result.columns
    True
    >>> (result['Predicted_COD'] >= 0).all()
    True
    """

doctest.testmod()